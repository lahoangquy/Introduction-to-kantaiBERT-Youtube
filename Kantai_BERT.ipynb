{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kantai BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "KantaiBERT is a Robustly Optimized BERT pretraining Approach (RoBERTa)-like model based on the architecture of BERT.\n",
        "\n",
        "The initial BERT models brought innovative features to the initial transformer models as we saw before. RoBERTa increases the performance of transformers for downstream tasks by improving the mechanics of the pretraining process. For instance, it does not use WordPiece tokenization but goes down to byte-level-Byte-Pair Encoding (BPE). This method paved the way for a wide variety of BERT and BERT like models.\n",
        "\n",
        "KantaiBERT like BERT will be trained using masked Langauge modeling (MLM). MLM is a langauge modelling technique that masks a word in a sequence. The transformer model must train to predict the masked word.\n",
        "\n",
        "KantaiBERT will be trained as a small model with 6 layers, 12 heads and 84,095,008 parameters. It might seem that 84 million parameters is a lot, but the parameters are spread over 12 heads which makes it a small model. A small model will make the pretraining experience smooth so that each step can be viewed in real time without waiting for hours to see the result.\n",
        "\n",
        "KantaiBERT is a GistilBERT-like bodel because it has the same architecture of 6 layers and 12 heads. DistilBERT is a distilled version of BERT. DistilBERT consists of fewer parameters than a RoBERTamodel. So it will run much faster but the result might be a bit less accurate than with a RoBERTa model."
      ],
      "metadata": {
        "id": "JRUQ_bvBHcPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tPxgwheBFLX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}