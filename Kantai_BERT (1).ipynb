{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kantai BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "KantaiBERT is a Robustly Optimized BERT pretraining Approach (RoBERTa)-like model based on the architecture of BERT.\n",
        "\n",
        "The initial BERT models brought innovative features to the initial transformer models as we saw before. RoBERTa increases the performance of transformers for downstream tasks by improving the mechanics of the pretraining process. For instance, it does not use WordPiece tokenization but goes down to byte-level-Byte-Pair Encoding (BPE). This method paved the way for a wide variety of BERT and BERT like models.\n",
        "\n",
        "KantaiBERT like BERT will be trained using masked Langauge modeling (MLM). MLM is a langauge modelling technique that masks a word in a sequence. The transformer model must train to predict the masked word.\n",
        "\n",
        "KantaiBERT will be trained as a small model with 6 layers, 12 heads and 84,095,008 parameters. It might seem that 84 million parameters is a lot, but the parameters are spread over 12 heads which makes it a small model. A small model will make the pretraining experience smooth so that each step can be viewed in real time without waiting for hours to see the result.\n",
        "\n",
        "KantaiBERT is a GistilBERT-like bodel because it has the same architecture of 6 layers and 12 heads. DistilBERT is a distilled version of BERT. DistilBERT consists of fewer parameters than a RoBERTamodel. So it will run much faster but the result might be a bit less accurate than with a RoBERTa model.\n",
        "\n",
        "We all know that large models achieve excellent performance. But what if we want to run a model on a smartphone. Miniaturization has been the key to technological evolution. Transformers will sometimes have to follow the same path during implementation. The Hugging Face approach using a distilled version of BERT is thus a good step for that. Distillation using few parameters or other such methods is a clever way of taking the best of pretraining and making it efficient for the needs of many downstream task. \n",
        "\n",
        "KantaiBERT will implement a byte-level byte-pair encoding tokenizer like the one usd by GPT-2. The special tokens will be the ones used by RoBERTa. BERT models most often use a WordPiece tokenizer. KantaiBERT will use a custom dataset, train a tokenizer, tran the transformer model, save it and run it with an MLM example."
      ],
      "metadata": {
        "id": "JRUQ_bvBHcPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tPxgwheBFLX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}